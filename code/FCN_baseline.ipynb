{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchical bert with linear layer model  both bert and linear layers are trained\n",
    "# 5*bert + linear layer 5*768-50-sigmoid  lr=0.00001,no weights in loss function, no bias in linear layer       validation loss decrease to 44.3, validation f1 increase to 0.74  early stop at 11th/12th epoch\n",
    "# 5*bert + linear layer 5*768-50-sigmoid  lr=0.00001, weights in loss function, no bias in linear layer       validation loss decrease to 44.8, validation f1 increase to 0.73  early stop at 9th/10th epoch\n",
    "# 5*bert + linear layer 5*768-512-Gelu-50-sigmoid  lr=0.00001, no weights in loss function, no bias in linear layer       validation loss decrease to 45.0, validation f1 increase to 0.75  early stop at 10th epoch\n",
    "# # 5*bert + linear layer 5*768-512-Gelu-50-sigmoid  lr=0.00001, weights in loss function, no bias in linear layer       validation loss decrease to 45.0, validation f1 increase to 0.74  early stop at 10th epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/home/niur/venv_1/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/u/home/niur/venv_1/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchmetrics\n",
    "import ast\n",
    "import random\n",
    "import transformers\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "# import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(dataframe):\n",
    "    for i in range(dataframe.shape[0]):\n",
    "        dataframe['TEXT'].iloc[i] = re.sub(\"(Admission Date:)|(Discharge Date:)|(Service:)|(Date of Birth:)|(Sex:)|(Attending:)|(Provider:)|(Name:)|(Date/Time:)|(MD Phone:)|(Completed by:)|(Job#:)|(Dictated By:)\", \"\", dataframe['TEXT'].iloc[i]) \n",
    "        dataframe['TEXT'].iloc[i] = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', '', dataframe['TEXT'].iloc[i])  # delete [**      **]\n",
    "        dataframe['TEXT'].iloc[i] = re.sub('[0-9]{2}:[0-9]{2}', '', dataframe['TEXT'].iloc[i])  # delete 12:11\n",
    "        dataframe['TEXT'].iloc[i] = re.sub('[0-9]{2}:[0-9]{2}AM', '', dataframe['TEXT'].iloc[i])  # delete 12:11AM\n",
    "        dataframe['TEXT'].iloc[i] = re.sub('[0-9]{2}:[0-9]{2}PM', '', dataframe['TEXT'].iloc[i])  # delete 12:11PM\n",
    "        dataframe['TEXT'].iloc[i] = re.sub('==+', ' ', dataframe['TEXT'].iloc[i])   # delete redundant space ' '\n",
    "        dataframe['TEXT'].iloc[i] = re.sub(' +', ' ', dataframe['TEXT'].iloc[i])   # delete redundant space ' '\n",
    "        dataframe['TEXT'].iloc[i] = re.sub('\\n', ' ', dataframe['TEXT'].iloc[i]) # change \\n to ' '\n",
    "        dataframe['TEXT'].iloc[i] = re.sub('# *', ' ', dataframe['TEXT'].iloc[i])\n",
    "        # dataframe['TEXT'].iloc[i] = re.sub(' - ', ' ', dataframe['TEXT'].iloc[i])\n",
    "        # dataframe['TEXT'].iloc[i] = re.sub('- ', ' ', dataframe['TEXT'].iloc[i])\n",
    "        # dataframe['TEXT'].iloc[i] = re.sub(': *', ' ', dataframe['TEXT'].iloc[i])\n",
    "        # dataframe['TEXT'].iloc[i] = re.sub('[0-9]\\. ', ' ', dataframe['TEXT'].iloc[i])\n",
    "        # dataframe['TEXT'].iloc[i] = re.sub('\\* ', ' ', dataframe['TEXT'].iloc[i])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_icdstr_to_list(df):\n",
    "    all_icds = []\n",
    "    for i in list(df['ICD9_CODE']):\n",
    "        all_icds.append(ast.literal_eval(''.join(i)))\n",
    "    df['ICD9_CODE'] = all_icds\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_icd_labels(datafram):\n",
    "    disease_list = list(pd.read_csv('./data/ICD9_description_LONG_SHORT_diagnose.csv')['DIAGNOSIS CODE'])\n",
    "    label_list = []\n",
    "    for i in range(datafram.shape[0]):\n",
    "        label = torch.tensor([0]*14567, dtype=int)\n",
    "        for token in list(datafram['ICD9_CODE'].iloc[i]):\n",
    "            index = disease_list.index(token)\n",
    "            label[index] = int(1)\n",
    "        label_list.append(label)\n",
    "    label_tensor = torch.stack(label_list)\n",
    "\n",
    "    return label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_tokens(datafram, tokenizer):\n",
    "\n",
    "    input_ids_list = []\n",
    "\n",
    "    for i in range(datafram.shape[0]):\n",
    "\n",
    "        tokenizer_outputs = tokenizer(datafram['TEXT'].iloc[i], padding='max_length' ,truncation=True, max_length=4*512, return_tensors='pt')\n",
    "        bert_token_tensor = tokenizer_outputs['input_ids'].reshape((4,512))\n",
    "        bert_token_tensor[:,0] = 101\n",
    "        if bert_token_tensor.shape[0] > 1:\n",
    "            bert_token_tensor[:-1,-1] = 102\n",
    "\n",
    "        input_ids_list.append(bert_token_tensor)\n",
    "\n",
    "    input_ids_tensor = torch.stack(input_ids_list, dim=0)\n",
    "\n",
    "    return input_ids_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_top500 = pd.read_csv('./data/top500_datasets/trainset_top500_new.csv')\n",
    "df_validation_top500 = pd.read_csv('./data/top500_datasets/validationset_top500_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/home/niur/htc_mimic3/ipykernel_1146530/246453972.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['TEXT'].iloc[i] = re.sub(\"(Admission Date:)|(Discharge Date:)|(Service:)|(Date of Birth:)|(Sex:)|(Attending:)|(Provider:)|(Name:)|(Date/Time:)|(MD Phone:)|(Completed by:)|(Job#:)|(Dictated By:)\", \"\", dataframe['TEXT'].iloc[i])\n",
      "/u/home/niur/htc_mimic3/ipykernel_1146530/246453972.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['TEXT'].iloc[i] = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', '', dataframe['TEXT'].iloc[i])  # delete [**      **]\n",
      "/u/home/niur/htc_mimic3/ipykernel_1146530/246453972.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['TEXT'].iloc[i] = re.sub('[0-9]{2}:[0-9]{2}', '', dataframe['TEXT'].iloc[i])  # delete 12:11\n",
      "/u/home/niur/htc_mimic3/ipykernel_1146530/246453972.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['TEXT'].iloc[i] = re.sub('[0-9]{2}:[0-9]{2}AM', '', dataframe['TEXT'].iloc[i])  # delete 12:11AM\n",
      "/u/home/niur/htc_mimic3/ipykernel_1146530/246453972.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['TEXT'].iloc[i] = re.sub('[0-9]{2}:[0-9]{2}PM', '', dataframe['TEXT'].iloc[i])  # delete 12:11PM\n",
      "/u/home/niur/htc_mimic3/ipykernel_1146530/246453972.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['TEXT'].iloc[i] = re.sub('==+', ' ', dataframe['TEXT'].iloc[i])   # delete redundant space ' '\n",
      "/u/home/niur/htc_mimic3/ipykernel_1146530/246453972.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['TEXT'].iloc[i] = re.sub(' +', ' ', dataframe['TEXT'].iloc[i])   # delete redundant space ' '\n",
      "/u/home/niur/htc_mimic3/ipykernel_1146530/246453972.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['TEXT'].iloc[i] = re.sub('\\n', ' ', dataframe['TEXT'].iloc[i]) # change \\n to ' '\n",
      "/u/home/niur/htc_mimic3/ipykernel_1146530/246453972.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['TEXT'].iloc[i] = re.sub('# *', ' ', dataframe['TEXT'].iloc[i])\n"
     ]
    }
   ],
   "source": [
    "df_train_top500 = text_cleaning(df_train_top500)\n",
    "df_validation_top500 = text_cleaning(df_validation_top500)\n",
    "\n",
    "df_train_top500 = convert_icdstr_to_list(df_train_top500)\n",
    "df_validation_top500 = convert_icdstr_to_list(df_validation_top500)\n",
    "\n",
    "label_tensor_train = create_all_icd_labels(df_train_top500)\n",
    "label_tensor_validation = create_all_icd_labels(df_validation_top500)\n",
    "\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('./Model_Bio_Clinical_DS_BERT/Bio_DischargeSummary_BERT/hub/models--emilyalsentzer--Bio_Discharge_Summary_BERT/snapshots/affde836a50e4d333f15dae9270f5a856d59540b', ignore_mismatched_sizes=True)\n",
    "\n",
    "\n",
    "\n",
    "bert_token_tensor_train = create_text_tokens(df_train_top500, tokenizer)\n",
    "bert_token_tensor_validation = create_text_tokens(df_validation_top500, tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8496, 14567])\n",
      "torch.Size([1500, 14567])\n",
      "torch.Size([8496, 4, 512])\n",
      "torch.Size([1500, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "print(label_tensor_train.shape)\n",
    "print(label_tensor_validation.shape)\n",
    "print(bert_token_tensor_train.shape)\n",
    "print(bert_token_tensor_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, label_tensor, bert_token_tensor):\n",
    "        self.label_tensor = label_tensor\n",
    "        self.bert_token_tensor = bert_token_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.label_tensor.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.label_tensor[index], self.bert_token_tensor[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset_top500 = CreateDataset(label_tensor=label_tensor_train, bert_token_tensor=bert_token_tensor_train)\n",
    "validation_dataset_top500 = CreateDataset(label_tensor=label_tensor_validation, bert_token_tensor=bert_token_tensor_validation)\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "trainset = DataLoader(dataset=training_dataset_top500, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=8)\n",
    "validationset = DataLoader(dataset=validation_dataset_top500, batch_size=batch_size, shuffle=False, drop_last=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HIBERT_with_Linear(torch.torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HIBERT_with_Linear, self).__init__()\n",
    "        self.clinical_DS_bert =  transformers.BertModel.from_pretrained('./Model_Bio_Clinical_DS_BERT/Bio_DischargeSummary_BERT/hub/models--emilyalsentzer--Bio_Discharge_Summary_BERT/snapshots/affde836a50e4d333f15dae9270f5a856d59540b')\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(4*768, 6*768, bias=True)\n",
    "        self.activ1 = torch.nn.GELU()\n",
    "        self.linear2 = torch.nn.Linear(6*768, 8*768, bias=True)\n",
    "        self.activ2 = torch.nn.GELU()\n",
    "        self.linear3 = torch.nn.Linear(8*768, 10*768, bias=True)\n",
    "        self.activ3 = torch.nn.GELU()\n",
    "        self.linear4 = torch.nn.Linear(10*768, 14567, bias=False)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "\n",
    "        templist = []\n",
    "        for item in tokens:   # bs*4*512\n",
    "            single_output = self.clinical_DS_bert(item).last_hidden_state[:,0,:].reshape((1,4*768))  # (1,4*768)\n",
    "            templist.append(single_output)   # [(1,4*768) , (1,4*768) ......]\n",
    "        x = torch.cat(templist, dim=0)   # (bs, 4*768)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.activ1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activ2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.activ3(x)\n",
    "        x = self.linear4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./Model_Bio_Clinical_DS_BERT/Bio_DischargeSummary_BERT/hub/models--emilyalsentzer--Bio_Discharge_Summary_BERT/snapshots/affde836a50e4d333f15dae9270f5a856d59540b were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = HIBERT_with_Linear()\n",
    "model = model.to(device)\n",
    "\n",
    "LR = 1e-6\n",
    "criterion = torch.nn.BCELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(validationset):\n",
    "    model.eval()\n",
    "    print('Validation...')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss_validation = 0\n",
    "        predictions_AUROC = torch.tensor([],dtype=float).to(device)\n",
    "        predictions_f1 = torch.tensor([], dtype=int).to(device)\n",
    "        labels_0_1 = torch.tensor([], dtype=int).to(device)\n",
    "\n",
    "        for i, (label_tensor_validation, bert_token_tensor_validation) in enumerate(validationset):\n",
    "            label_tensor_validation, bert_token_tensor_validation = label_tensor_validation.to(device), bert_token_tensor_validation.to(device)\n",
    "            outputs = model(bert_token_tensor_validation)  # (bs,14567)\n",
    "            predictions_AUROC = torch.cat([predictions_AUROC, outputs])\n",
    "            \n",
    "            outputs = torch.nn.Sigmoid()(outputs)\n",
    "\n",
    "            valid_loss = criterion(outputs.float(), label_tensor_validation.float())\n",
    "            loss_validation += valid_loss.item()\n",
    "\n",
    "            outputs = torch.gt(outputs, 0.5)*int(1)\n",
    "            predictions_f1 = torch.cat([predictions_f1, outputs])\n",
    "            labels_0_1 = torch.cat([labels_0_1, label_tensor_validation])\n",
    "        \n",
    "        \n",
    "        valid_f1_micro = torchmetrics.functional.classification.multilabel_f1_score(predictions_f1, labels_0_1, num_labels=14567, threshold=0.5, average='micro', multidim_average='global')\n",
    "        valid_f1_weighted = torchmetrics.functional.classification.multilabel_f1_score(predictions_f1, labels_0_1, num_labels=14567, threshold=0.5, average='weighted', multidim_average='global')\n",
    "        valid_AUROC_micro = torchmetrics.functional.classification.multilabel_auroc(predictions_AUROC, labels_0_1, num_labels=14567, average='micro')\n",
    "        valid_AUROC_weighted = torchmetrics.functional.classification.multilabel_auroc(predictions_AUROC, labels_0_1, num_labels=14567, average='weighted')\n",
    "\n",
    "        valid_precision = torchmetrics.functional.classification.multilabel_precision(predictions_f1, labels_0_1, num_labels=14567,multidim_average='global', average='micro')\n",
    "        valid_recall = torchmetrics.functional.classification.multilabel_recall(predictions_f1, labels_0_1, num_labels=14567,multidim_average='global', average='micro')\n",
    "        valid_true_positive = torch.sum((predictions_f1+labels_0_1)==2)/labels_0_1.shape[0]/labels_0_1.shape[1]\n",
    "        valid_true_negative = torch.sum((predictions_f1+labels_0_1)==0)/labels_0_1.shape[0]/labels_0_1.shape[1]\n",
    "\n",
    "\n",
    "    \n",
    "    return loss_validation/df_validation_top500.shape[0], valid_f1_micro, valid_f1_weighted, valid_AUROC_micro, valid_AUROC_weighted, valid_precision, valid_recall, valid_true_positive, valid_true_negative\n",
    "    \n",
    "        \n",
    "def start_training(trainset, validationset, epochs=75):\n",
    "    # config = { 'batch_size': batch_size, 'lr':LR, 'loss':'BCE', 'optim':'Adam', 'comment':'4 layers of FCN'}\n",
    "    # wandb.init(project=\"FCN_baseline\", entity=\"htc-mimic3\", config=config)\n",
    "    \n",
    "    print('Training...')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch: {epoch+1}')\n",
    "        #TRAININING STEP\n",
    "\n",
    "        loss_training = 0.0\n",
    "        model.train()\n",
    "        for i, (label_tensor_train, bert_token_tensor_train) in enumerate(trainset):\n",
    "            label_tensor_train, bert_token_tensor_train = label_tensor_train.to(device), bert_token_tensor_train.to(device)\n",
    "            outputs = model(bert_token_tensor_train)  # bs*14567\n",
    "            outputs = torch.nn.Sigmoid()(outputs)\n",
    "\n",
    "            loss = criterion(outputs.float(), label_tensor_train.float())  # (bs*14567)  (bs*14567)\n",
    "\n",
    "            \n",
    "            #backward step\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()         \n",
    "            loss_training += loss.item()\n",
    "\n",
    "        \n",
    "        print('the training loss in epoch',epoch+1,'is:',loss_training/df_train_top500.shape[0])\n",
    "        loss_validation, valid_f1_micro, valid_f1_weighted, valid_AUROC_micro, valid_AUROC_weighted, valid_precision, valid_recall, valid_true_positive, valid_true_negative = valid(validationset=validationset)\n",
    "        \n",
    "        print('the loss_validation in epoch',epoch+1,'is:',loss_validation)\n",
    "        print('the valid_f1_micro in epoch',epoch+1,'is:',valid_f1_micro)\n",
    "        print('the valid_f1_weighted in epoch',epoch+1,'is:',valid_f1_weighted)\n",
    "        print('the valid_AUROC_micro in epoch',epoch+1,'is:',valid_AUROC_micro)\n",
    "        print('the valid_AUROC_weighted in epoch',epoch+1,'is:',valid_AUROC_weighted)\n",
    "        print('the valid_precision in epoch',epoch+1,'is:',valid_precision)\n",
    "        print('the valid_recall in epoch',epoch+1,'is:',valid_recall)\n",
    "        print('the valid_true_positive in epoch',epoch+1,'is:',valid_true_positive)\n",
    "        print('the valid_true_negative in epoch',epoch+1,'is:',valid_true_negative)\n",
    "\n",
    "        # wandb.log({'training_loss': loss_training/df_train_top500.shape[0]})\n",
    "        # wandb.log({'loss_validation': loss_validation})\n",
    "        # wandb.log({'valid_f1_micro': valid_f1_micro})\n",
    "        # wandb.log({'valid_f1_weighted': valid_f1_weighted})\n",
    "        # wandb.log({'valid_AUROC_micro': valid_AUROC_micro})\n",
    "        # wandb.log({'valid_AUROC_weighted': valid_AUROC_weighted})\n",
    "        # wandb.log({'valid_precision': valid_precision})\n",
    "        # wandb.log({'valid_recall': valid_recall})\n",
    "        # wandb.log({'valid_true_positive': valid_true_positive})\n",
    "        # wandb.log({'valid_true_negative': valid_true_negative})\n",
    "\n",
    "\n",
    "\n",
    "        print('EPOCH DONE!')\n",
    "    print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/u/home/niur/htc_mimic3/FCN_baseline.ipynb Cell 15\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Balpaca.aim.cit.tum.de/u/home/niur/htc_mimic3/FCN_baseline.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m start_training(trainset\u001b[39m=\u001b[39;49mtrainset, validationset\u001b[39m=\u001b[39;49mvalidationset)\n",
      "\u001b[1;32m/u/home/niur/htc_mimic3/FCN_baseline.ipynb Cell 15\u001b[0m in \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Balpaca.aim.cit.tum.de/u/home/niur/htc_mimic3/FCN_baseline.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m#backward step\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Balpaca.aim.cit.tum.de/u/home/niur/htc_mimic3/FCN_baseline.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Balpaca.aim.cit.tum.de/u/home/niur/htc_mimic3/FCN_baseline.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Balpaca.aim.cit.tum.de/u/home/niur/htc_mimic3/FCN_baseline.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()         \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Balpaca.aim.cit.tum.de/u/home/niur/htc_mimic3/FCN_baseline.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m loss_training \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/venv_1/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/venv_1/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_training(trainset=trainset, validationset=validationset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv_1': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2987113b9c300c1ae8cab2be01e277860fb941e8f6dd0f5e5312227dbdee95ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
